{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Challenge Técnico para la posición de Data Engineer LATAM.\n",
    "\n",
    "<p> Creacion de notebook que permita realizar el Challenge tecnico para la posición Data Engineer.\n",
    " Este notebook se puede ejecutar las veces que sea necesario y estaria listo para pasar a producccion sin errores </p>\n",
    " \n",
    " ##### Resumen:\n",
    "\n",
    " <p> Se realizara la implementación de los siguientes puntos:\n",
    " \n",
    " * Las top 10 fechas donde hay más tweets. Mencionar el usuario (username) que más publicaciones tiene por cada uno de esos días. \n",
    " * Los top 10 emojis más usados con su respectivo conteo.\n",
    " * El top 10 histórico de usuarios (username) más influyentes en función del conteo de las menciones (@) que registra cada uno de ellos.\n",
    "  </p>\n",
    "\n",
    "##### Objetivo:\n",
    "\n",
    "<p> Creacion de notbook que me permita realizar la prueba de Challenge tecnico para la posición Data Engineer - LATAM. </p>\n",
    "\n",
    "##### Resultados:\n",
    "\n",
    "<p> Resolucion de Challenge tecnico para la posición Data Engineer </p>\n",
    "\n",
    "##### Autor:\n",
    "\n",
    "<p> Teddy Arteaga </p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "\n",
    "\n",
    "Variables globales: se crea variables globales con el fin de que puedar se reutilizadas: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_path = \"../data/farmers-protest-tweets-2021-2-4.json\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Las top 10 fechas donde hay más tweets. Mencionar el usuario (username) que más publicaciones tiene por cada uno de esos días. \n",
    "\n",
    "Por temas de que se note las librerias que se utilizan en cada función, se van a ir colocando.\n",
    "Para la primera función *q1_time* se esta priorizando el tiempo de ejecución a continuación detallo lo que se esta realizando:\n",
    "\n",
    "* Para procesar la información utilizaremos pandas que es una muy buena opción debido a su versatilidad.\n",
    "* Como buena practica utilizamos el uso de exceptiones con el fin de poder manejar FileNotFoundError cuando el archivo no se encuentre.\n",
    "* En lugar de leer el archivo JSON y luego convertirlo a Parquet, la función intenta leer directamente el archivo Parquet si este ya existe\n",
    "* No realizamos ninguna operación que afecte al tiempo de ejecución(Eliminación de columnas, conversión en todos los datos, etc) hacemos el conteno directamente en los datos que son relevantes\n",
    "* Para cada fecha con más tweets, se utiliza nlargest(1, 'count') en el DataFrame date_user_counts para obtener directamente el usuario con más tweets para esa fecha, en lugar de filtrar el DataFrame repetidamente."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from typing import List, Tuple\n",
    "from datetime import date\n",
    "\n",
    "def q1_time(file_path: str) -> List[Tuple[date, str]]:\n",
    "    # Leer el archivo JSON directamente si el archivo Parquet no existe\n",
    "    try:\n",
    "        df = pd.read_parquet(file_path.replace('.json', '.parquet'))\n",
    "    except FileNotFoundError:\n",
    "        df = pd.read_json(file_path, lines=True)\n",
    "        df['date'] = pd.to_datetime(df['date']).dt.date\n",
    "        df['username'] = df['user'].apply(lambda x: x.get('username'))\n",
    "        df.drop(columns=['user'], inplace=True)\n",
    "        df.to_parquet(file_path.replace('.json', '.parquet'), index=False)\n",
    "\n",
    "    # Contar el número de tweets por fecha y usuario\n",
    "    date_user_counts = df.groupby(['date', 'username']).size().reset_index(name='count')\n",
    "\n",
    "    # Encontrar las 10 fechas con más tweets\n",
    "    top_dates = df['date'].value_counts().nlargest(10).index\n",
    "\n",
    "    # Obtener el usuario con más tweets para cada una de las 10 fechas\n",
    "    result = []\n",
    "    for date_val in top_dates:\n",
    "        top_user_df = date_user_counts[date_user_counts['date'] == date_val].nlargest(1, 'count')\n",
    "        top_user = top_user_df['username'].iloc[0]\n",
    "        result.append((date_val, top_user))\n",
    "\n",
    "    return result\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Optimización en el uso de la memoria:\n",
    "Para este caso vamos a leer directamente el archivo json.\n",
    "* Con top_users_by_date mantenemos un diccionario con el fin de mantener los usuarios con mas tweets\n",
    "* Eliminamos date_counts ya que no se utiliza para los datos finales\n",
    "* Se optimiza la actualización del diccionario top_users_by_date para mantener solo la información necesaria para obtener el usuario con más tweets por fecha."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "from typing import List, Tuple\n",
    "from collections import defaultdict\n",
    "import ujson as json\n",
    "\n",
    "def q1_memory(file_path: str) -> List[Tuple[datetime.date, str]]:\n",
    "    # Diccionario para mantener el conteo de tweets por fecha\n",
    "    date_counts = defaultdict(int)\n",
    "    # Diccionario para mantener el usuario con más tweets por fecha\n",
    "    top_users_by_date = {}\n",
    "    \n",
    "    with open(file_path, \"r\") as file:\n",
    "        for line in file:\n",
    "            tweet = json.loads(line)\n",
    "            # Se extrae la fecha del tweet y se convierte a datetime.date\n",
    "            tweet_date = datetime.strptime(tweet['date'].split(\"T\")[0], '%Y-%m-%d').date()\n",
    "            \n",
    "            # Se actualiza el conteo de tweets para esa fecha\n",
    "            date_counts[tweet_date] += 1\n",
    "            \n",
    "            # Se actualiza el usuario con más tweets para esa fecha\n",
    "            if tweet_date not in top_users_by_date:\n",
    "                top_users_by_date[tweet_date] = defaultdict(int)\n",
    "            top_users_by_date[tweet_date][tweet['user']['username']] += 1\n",
    "    \n",
    "    # Se obtienen las top 10 fechas con más tweets\n",
    "    top_dates = sorted(date_counts, key=date_counts.get, reverse=True)[:10]\n",
    "    \n",
    "    # Se obtiene el usuario con más tweets para cada fecha\n",
    "    result = [(date, max(top_users_by_date[date], key=top_users_by_date[date].get)) for date in top_dates]\n",
    "    \n",
    "    return result\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Los top 10 emojis más usados con su respectivo conteo. \n",
    "\n",
    "Enfoque de tiempo de ejecución"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import emoji\n",
    "from collections import Counter\n",
    "from typing import List, Tuple\n",
    "\n",
    "def q2_time(file_path: str) -> List[Tuple[str, int]]:\n",
    "    # Se ajusta la ruta de lectura de archivo json a .parquet para futuras lecturas\n",
    "    parquet_file = file_path.replace('.json', '.parquet')\n",
    "    \n",
    "    try:\n",
    "        # Se intenta leer el archivo parquet\n",
    "        df = pd.read_parquet(parquet_file)\n",
    "    except FileNotFoundError:\n",
    "        # Si el archivo parquet no existe, se lee el archivo json y se convierte a parquet\n",
    "        df = pd.read_json(file_path, lines=True)\n",
    "        df.to_parquet(parquet_file, index=False)\n",
    "    \n",
    "    # Se extraen todos los emojis de la columna 'content' y se cuenta su frecuencia\n",
    "    all_emojis = []\n",
    "    for content in df['content']:\n",
    "        emojis_in_content = [entry['emoji'] for entry in emoji.emoji_list(content)]\n",
    "        all_emojis.extend(emojis_in_content)\n",
    "    # Se cuenta la frecuencia de cada emoji\n",
    "    emoji_counts = Counter(all_emojis)\n",
    "\n",
    "    # Se obtienen los top 10 emojis más utilizados\n",
    "    top_emojis = emoji_counts.most_common(10)\n",
    "    \n",
    "    return top_emojis\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Enfoque de memoria optimatizada"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import ujson as json\n",
    "from collections import Counter\n",
    "from typing import List, Tuple\n",
    "import emoji\n",
    "\n",
    "def q2_memory(file_path: str) -> List[Tuple[str, int]]:\n",
    "    # Se crea un contador para contar la frecuencia de cada emoji\n",
    "    emoji_counts = Counter()\n",
    "    \n",
    "    # Se abre el archivo JSON y se itera sobre cada línea\n",
    "    with open(file_path, 'r', encoding='utf-8') as file:\n",
    "        for line in file:\n",
    "            # Se convierte la línea en un diccionario\n",
    "            tweet = json.loads(line)\n",
    "            # Se extrae el contenido del tweet y se obtienen los emojis\n",
    "            content = tweet.get('content', '')\n",
    "            emojis_in_content = [entry['emoji'] for entry in emoji.emoji_list(content)]\n",
    "            \n",
    "            # Se actualiza el contador con los emojis encontrados en este tweet\n",
    "            emoji_counts.update(emojis_in_content)\n",
    "\n",
    "    # Se obtienen los top 10 emojis más utilizados\n",
    "    top_emojis = emoji_counts.most_common(10)\n",
    "    \n",
    "    return top_emojis\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### El top 10 histórico de usuarios (username) más influyentes en función del conteo de las menciones (@) que registra cada uno de ellos.\n",
    "\n",
    "Enfoque tiempo de ejecución."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "from collections import Counter\n",
    "from typing import List, Tuple\n",
    "\n",
    "def q3_time(file_path: str) -> List[Tuple[str, int]]:\n",
    "    # Se ajusta la ruta de lectura del archivo JSON a .parquet para futuras lecturas\n",
    "    parquet_file = file_path.replace('.json', '.parquet')\n",
    "\n",
    "    try:\n",
    "        # Intentar leer el archivo Parquet si existe\n",
    "        df = pd.read_parquet(parquet_file)\n",
    "    except FileNotFoundError:\n",
    "        try:\n",
    "            # Intentar leer el archivo JSON y convertirlo a DataFrame\n",
    "            df = pd.read_json(file_path, lines=True)\n",
    "            # Procesar los datos y guardarlos como archivo Parquet\n",
    "            df.to_parquet(parquet_file, index=False)\n",
    "        except Exception as e:\n",
    "            print(f\"Error al procesar el archivo: {e}\")\n",
    "            return []\n",
    "\n",
    "    # Se extraen todas las menciones por cada tweet\n",
    "    mentions_flat = []\n",
    "    for mentions_list in df['content'].str.findall(r'@(\\w+)').dropna():\n",
    "        mentions_flat.extend(mentions_list)\n",
    "\n",
    "    # Se cuenta la frecuencia de cada mención en la lista aplanada\n",
    "    mention_counts = Counter(mentions_flat)\n",
    "\n",
    "    # Obtener los top 10 usuarios más mencionados\n",
    "    top_mentions = mention_counts.most_common(10)\n",
    "\n",
    "    return top_mentions\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Enfoque de memoria optomatizada"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import ujson as json\n",
    "from collections import Counter\n",
    "from typing import List, Tuple\n",
    "import re\n",
    "\n",
    "def q3_memory(file_path: str) -> List[Tuple[str, int]]:\n",
    "    # Se crea un Counter para contar la frecuencia de cada mención\n",
    "    mention_counts = Counter()\n",
    "    \n",
    "    # Se abre el archivo json y se itera sobre cada tweet\n",
    "    with open(file_path, 'r') as file:\n",
    "        # Iterar línea por línea en el archivo\n",
    "        for line in file:\n",
    "            tweet = json.loads(line)\n",
    "            content = tweet.get('content', '')\n",
    "            # Se busca todas las menciones en el contenido y se actualiza el Counter\n",
    "            mention_counts.update(re.findall(r'@(\\w+)', content))\n",
    "            \n",
    "    # Obtener los top 10 usuarios más mencionados\n",
    "    top_mentions = mention_counts.most_common(10)\n",
    "    \n",
    "    return top_mentions\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Resultados\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(datetime.date(2021, 2, 12), 'RanbirS00614606'),\n",
       " (datetime.date(2021, 2, 13), 'MaanDee08215437'),\n",
       " (datetime.date(2021, 2, 17), 'RaaJVinderkaur'),\n",
       " (datetime.date(2021, 2, 16), 'jot__b'),\n",
       " (datetime.date(2021, 2, 14), 'rebelpacifist'),\n",
       " (datetime.date(2021, 2, 18), 'neetuanjle_nitu'),\n",
       " (datetime.date(2021, 2, 15), 'jot__b'),\n",
       " (datetime.date(2021, 2, 20), 'MangalJ23056160'),\n",
       " (datetime.date(2021, 2, 23), 'Surrypuria'),\n",
       " (datetime.date(2021, 2, 19), 'Preetm91')]"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "q1_time(file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(datetime.date(2021, 2, 12), 'RanbirS00614606'),\n",
       " (datetime.date(2021, 2, 13), 'MaanDee08215437'),\n",
       " (datetime.date(2021, 2, 17), 'RaaJVinderkaur'),\n",
       " (datetime.date(2021, 2, 16), 'jot__b'),\n",
       " (datetime.date(2021, 2, 14), 'rebelpacifist'),\n",
       " (datetime.date(2021, 2, 18), 'neetuanjle_nitu'),\n",
       " (datetime.date(2021, 2, 15), 'jot__b'),\n",
       " (datetime.date(2021, 2, 20), 'MangalJ23056160'),\n",
       " (datetime.date(2021, 2, 23), 'Surrypuria'),\n",
       " (datetime.date(2021, 2, 19), 'Preetm91')]"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "q1_memory(file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('🙏', 5049),\n",
       " ('😂', 3072),\n",
       " ('🚜', 2972),\n",
       " ('🌾', 2182),\n",
       " ('🇮🇳', 2086),\n",
       " ('🤣', 1668),\n",
       " ('✊', 1651),\n",
       " ('❤️', 1382),\n",
       " ('🙏🏻', 1317),\n",
       " ('💚', 1040)]"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "q2_time(file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('🙏', 5049),\n",
       " ('😂', 3072),\n",
       " ('🚜', 2972),\n",
       " ('🌾', 2182),\n",
       " ('🇮🇳', 2086),\n",
       " ('🤣', 1668),\n",
       " ('✊', 1651),\n",
       " ('❤️', 1382),\n",
       " ('🙏🏻', 1317),\n",
       " ('💚', 1040)]"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "q2_memory(file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('narendramodi', 2261),\n",
       " ('Kisanektamorcha', 1836),\n",
       " ('RakeshTikaitBKU', 1639),\n",
       " ('PMOIndia', 1422),\n",
       " ('RahulGandhi', 1125),\n",
       " ('GretaThunberg', 1046),\n",
       " ('RaviSinghKA', 1015),\n",
       " ('rihanna', 972),\n",
       " ('UNHumanRights', 962),\n",
       " ('meenaharris', 925)]"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "q3_time(file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('narendramodi', 2261),\n",
       " ('Kisanektamorcha', 1836),\n",
       " ('RakeshTikaitBKU', 1639),\n",
       " ('PMOIndia', 1422),\n",
       " ('RahulGandhi', 1125),\n",
       " ('GretaThunberg', 1046),\n",
       " ('RaviSinghKA', 1015),\n",
       " ('rihanna', 972),\n",
       " ('UNHumanRights', 962),\n",
       " ('meenaharris', 925)]"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "q3_memory(  )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Mejoras referente a la carga del archivo\n",
    "\n",
    "* Si el archivo sigue creciendo sugiero dividir el archivo en lotes con el fin que pueda caber en la memoria RAM, leer y procesar un número limitado de registros a la vez en lugar de cargar todo el archivo en la memoria de una vez.\n",
    "* Tener una estructura de datos eficionte lo que nos ayude automatizar el tamaño del archivo. Por ejemplo que el archivo venga directamente en parquet debido su alamacenamiento columnar lo que reduciria considerablemente el archivo."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pruebas de las funciones"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluacion de memoria q1_memory\n",
      "ERROR: Could not find file C:\\Users\\teddy\\AppData\\Local\\Temp\\ipykernel_90700\\2435509172.py\n",
      "Sun Mar 17 20:51:21 2024    q1_time_stats.pstats\n",
      "\n",
      "         25675 function calls (25107 primitive calls) in 1.083 seconds\n",
      "\n",
      "   Ordered by: cumulative time\n",
      "   List reduced from 684 to 10 due to restriction <10>\n",
      "\n",
      "   ncalls  tottime  percall  cumtime  percall filename:lineno(function)\n",
      "        1    0.000    0.000    1.083    1.083 C:\\Users\\teddy\\AppData\\Local\\Temp\\ipykernel_90700\\1997928156.py:5(q1_time)\n",
      "        1    0.001    0.001    1.022    1.022 c:\\Users\\teddy\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\pandas\\io\\parquet.py:501(read_parquet)\n",
      "        1    0.000    0.000    1.020    1.020 c:\\Users\\teddy\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\pandas\\io\\parquet.py:237(read)\n",
      "        1    0.000    0.000    0.746    0.746 c:\\Users\\teddy\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\pyarrow\\pandas_compat.py:752(table_to_dataframe)\n",
      "        1    0.744    0.744    0.744    0.744 c:\\Users\\teddy\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\pyarrow\\pandas_compat.py:1126(_table_to_blocks)\n",
      "        1    0.000    0.000    0.274    0.274 c:\\Users\\teddy\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\pyarrow\\parquet\\core.py:1759(read_table)\n",
      "        1    0.271    0.271    0.271    0.271 c:\\Users\\teddy\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\pyarrow\\parquet\\core.py:1403(read)\n",
      "        1    0.000    0.000    0.025    0.025 c:\\Users\\teddy\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\pandas\\core\\groupby\\groupby.py:2902(size)\n",
      "        1    0.000    0.000    0.025    0.025 c:\\Users\\teddy\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\pandas\\core\\groupby\\ops.py:684(size)\n",
      "        1    0.000    0.000    0.023    0.023 c:\\Users\\teddy\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\pandas\\core\\groupby\\ops.py:727(group_info)\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from memory_profiler import profile\n",
    "import cProfile\n",
    "import pstats\n",
    "from datetime import datetime\n",
    "\n",
    "# Define tus funciones q1_memory y q1_time aquí\n",
    "\n",
    "@profile\n",
    "def test_memory_usage(file_path):\n",
    "    # Llama a tus funciones aquí\n",
    "    q1_memory(file_path)\n",
    "    q1_time(file_path)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "   \n",
    "    # Prueba de q1_memory\n",
    "    print(\"Evaluacion de memoria q1_memory\")\n",
    "    test_memory_usage(file_path)\n",
    "    \n",
    "    # Prueba de q1_time\n",
    "    profiler = cProfile.Profile()\n",
    "    profiler.enable()\n",
    "    # Codigo a evaluar\n",
    "    q1_time(file_path)\n",
    "    profiler.disable()\n",
    "    profiler.dump_stats(\"q1_time_stats.pstats\")\n",
    "    stats = pstats.Stats(\"q1_time_stats.pstats\")\n",
    "    stats.sort_stats(\"cumulative\")\n",
    "    # Se muestran resultados ordenados por tiempo de ejecucion, del mas lento al mas rapido\n",
    "    stats.print_stats(10)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluacion de memoria q1_memory\n",
      "ERROR: Could not find file C:\\Users\\teddy\\AppData\\Local\\Temp\\ipykernel_90700\\1703600861.py\n",
      "Sun Mar 17 21:09:49 2024    q2_time_stats.pstats\n",
      "\n",
      "         172298652 function calls (172298634 primitive calls) in 37.359 seconds\n",
      "\n",
      "   Ordered by: cumulative time\n",
      "   List reduced from 379 to 10 due to restriction <10>\n",
      "\n",
      "   ncalls  tottime  percall  cumtime  percall filename:lineno(function)\n",
      "   234814    6.043    0.000   34.362    0.000 c:\\Users\\teddy\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\emoji\\core.py:283(emoji_list)\n",
      "      7/3    0.046    0.007   26.366    8.789 c:\\Users\\teddy\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\asyncio\\base_events.py:1908(_run_once)\n",
      " 34281412   16.141    0.000   26.237    0.000 c:\\Users\\teddy\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\emoji\\tokenizer.py:158(tokenize)\n",
      "        8    0.063    0.008   21.014    2.627 c:\\Users\\teddy\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\selectors.py:313(_select)\n",
      "        7    0.555    0.079   18.093    2.585 {built-in method select.select}\n",
      " 34046604    4.167    0.000    7.284    0.000 <string>:1(<lambda>)\n",
      " 34046615    3.117    0.000    3.117    0.000 {built-in method __new__ of type object at 0x00007FFF43B298B0}\n",
      " 34046611    2.765    0.000    2.765    0.000 {method 'append' of 'list' objects}\n",
      "34164276/34164274    2.111    0.000    2.151    0.000 {built-in method builtins.isinstance}\n",
      "        1    0.006    0.006    1.861    1.861 C:\\Users\\teddy\\AppData\\Local\\Temp\\ipykernel_90700\\81624651.py:7(q2_time)\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Define tus funciones q1_memory y q1_time aquí\n",
    "\n",
    "@profile\n",
    "def test_memory_usage(file_path):\n",
    "    # Llama a tus funciones aquí\n",
    "    q2_memory(file_path)\n",
    "    q2_time(file_path)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "   \n",
    "    # Prueba de q1_memory\n",
    "    print(\"Evaluacion de memoria q1_memory\")\n",
    "    test_memory_usage(file_path)\n",
    "    \n",
    "    # Prueba de q1_time\n",
    "    profiler = cProfile.Profile()\n",
    "    profiler.enable()\n",
    "    # Codigo a evaluar\n",
    "    q2_time(file_path)\n",
    "    q2_memory(file_path)\n",
    "    profiler.disable()\n",
    "    profiler.dump_stats(\"q2_time_stats.pstats\")\n",
    "    stats = pstats.Stats(\"q2_time_stats.pstats\")\n",
    "    stats.sort_stats(\"cumulative\")\n",
    "    # Se muestran resultados ordenados por tiempo de ejecucion, del mas lento al mas rapido\n",
    "    stats.print_stats(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluacion de memoria q1_memory\n",
      "ERROR: Could not find file C:\\Users\\teddy\\AppData\\Local\\Temp\\ipykernel_90700\\2046086110.py\n",
      "Sun Mar 17 21:10:28 2024    q3_time_stats.pstats\n",
      "\n",
      "         1510390 function calls (1510372 primitive calls) in 3.683 seconds\n",
      "\n",
      "   Ordered by: cumulative time\n",
      "   List reduced from 443 to 10 due to restriction <10>\n",
      "\n",
      "   ncalls  tottime  percall  cumtime  percall filename:lineno(function)\n",
      "        1    0.012    0.012    2.059    2.059 C:\\Users\\teddy\\AppData\\Local\\Temp\\ipykernel_90700\\1467250520.py:6(q3_time)\n",
      "        1    0.001    0.001    1.992    1.992 c:\\Users\\teddy\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\pandas\\io\\parquet.py:501(read_parquet)\n",
      "        1    0.000    0.000    1.991    1.991 c:\\Users\\teddy\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\pandas\\io\\parquet.py:237(read)\n",
      "        1    0.000    0.000    1.587    1.587 c:\\Users\\teddy\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\pyarrow\\pandas_compat.py:752(table_to_dataframe)\n",
      "        1    1.586    1.586    1.586    1.586 c:\\Users\\teddy\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\pyarrow\\pandas_compat.py:1126(_table_to_blocks)\n",
      "   117407    0.991    0.000    0.991    0.000 {built-in method ujson.loads}\n",
      "        1    0.000    0.000    0.403    0.403 c:\\Users\\teddy\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\pyarrow\\parquet\\core.py:1759(read_table)\n",
      "        1    0.400    0.400    0.400    0.400 c:\\Users\\teddy\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\pyarrow\\parquet\\core.py:1403(read)\n",
      "        2    0.000    0.000    0.379    0.190 c:\\Users\\teddy\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\asyncio\\base_events.py:1908(_run_once)\n",
      "        2    0.000    0.000    0.379    0.189 c:\\Users\\teddy\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\selectors.py:319(select)\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Define tus funciones q1_memory y q1_time aquí\n",
    "\n",
    "@profile\n",
    "def test_memory_usage(file_path):\n",
    "    # Llama a tus funciones aquí\n",
    "    q3_memory(file_path)\n",
    "    q3_time(file_path)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "   \n",
    "    # Prueba de q1_memory\n",
    "    print(\"Evaluacion de memoria q1_memory\")\n",
    "    test_memory_usage(file_path)\n",
    "    \n",
    "    # Prueba de q1_time\n",
    "    profiler = cProfile.Profile()\n",
    "    profiler.enable()\n",
    "    # Codigo a evaluar\n",
    "    q3_time(file_path)\n",
    "    q3_memory(file_path)\n",
    "    profiler.disable()\n",
    "    profiler.dump_stats(\"q3_time_stats.pstats\")\n",
    "    stats = pstats.Stats(\"q3_time_stats.pstats\")\n",
    "    stats.sort_stats(\"cumulative\")\n",
    "    # Se muestran resultados ordenados por tiempo de ejecucion, del mas lento al mas rapido\n",
    "    stats.print_stats(10)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
