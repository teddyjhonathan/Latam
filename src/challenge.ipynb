{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Challenge T√©cnico para la posici√≥n de Data Engineer LATAM.\n",
    "\n",
    "<p> Creacion de notebook que permita realizar el Challenge tecnico para la posici√≥n Data Engineer.\n",
    " Este notebook se puede ejecutar las veces que sea necesario y estaria listo para pasar a producccion sin errores </p>\n",
    " \n",
    " ##### Resumen:\n",
    "\n",
    " <p> Se realizara la implementaci√≥n de los siguientes puntos:\n",
    " \n",
    " * Las top 10 fechas donde hay m√°s tweets. Mencionar el usuario (username) que m√°s publicaciones tiene por cada uno de esos d√≠as. \n",
    " * Los top 10 emojis m√°s usados con su respectivo conteo.\n",
    " * El top 10 hist√≥rico de usuarios (username) m√°s influyentes en funci√≥n del conteo de las menciones (@) que registra cada uno de ellos.\n",
    "  </p>\n",
    "\n",
    "##### Objetivo:\n",
    "\n",
    "<p> Creacion de notbook que me permita realizar la prueba de Challenge tecnico para la posici√≥n Data Engineer - LATAM. </p>\n",
    "\n",
    "##### Resultados:\n",
    "\n",
    "<p> Resolucion de Challenge tecnico para la posici√≥n Data Engineer </p>\n",
    "\n",
    "##### Autor:\n",
    "\n",
    "<p> Teddy Arteaga </p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "\n",
    "\n",
    "Variables globales: se crea variables globales con el fin de que puedar se reutilizadas: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_path = \"../data/farmers-protest-tweets-2021-2-4.json\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Las top 10 fechas donde hay m√°s tweets. Mencionar el usuario (username) que m√°s publicaciones tiene por cada uno de esos d√≠as. \n",
    "\n",
    "Por temas de que se note las librerias que se utilizan en cada funci√≥n, se van a ir colocando.\n",
    "Para la primera funci√≥n *q1_time* se esta priorizando el tiempo de ejecuci√≥n a continuaci√≥n detallo lo que se esta realizando:\n",
    "\n",
    "* Para procesar la informaci√≥n utilizaremos pandas que es una muy buena opci√≥n debido a su versatilidad.\n",
    "* Como buena practica utilizamos el uso de exceptiones con el fin de poder manejar FileNotFoundError cuando el archivo no se encuentre.\n",
    "* En lugar de leer el archivo JSON y luego convertirlo a Parquet, la funci√≥n intenta leer directamente el archivo Parquet si este ya existe\n",
    "* No realizamos ninguna operaci√≥n que afecte al tiempo de ejecuci√≥n(Eliminaci√≥n de columnas, conversi√≥n en todos los datos, etc) hacemos el conteno directamente en los datos que son relevantes\n",
    "* Para cada fecha con m√°s tweets, se utiliza nlargest(1, 'count') en el DataFrame date_user_counts para obtener directamente el usuario con m√°s tweets para esa fecha, en lugar de filtrar el DataFrame repetidamente."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from typing import List, Tuple\n",
    "from datetime import date\n",
    "\n",
    "def q1_time(file_path: str) -> List[Tuple[date, str]]:\n",
    "    # Leer el archivo JSON directamente si el archivo Parquet no existe\n",
    "    try:\n",
    "        df = pd.read_parquet(file_path.replace('.json', '.parquet'))\n",
    "    except FileNotFoundError:\n",
    "        df = pd.read_json(file_path, lines=True)\n",
    "        df['date'] = pd.to_datetime(df['date']).dt.date\n",
    "        df['username'] = df['user'].apply(lambda x: x.get('username'))\n",
    "        df.drop(columns=['user'], inplace=True)\n",
    "        df.to_parquet(file_path.replace('.json', '.parquet'), index=False)\n",
    "\n",
    "    # Contar el n√∫mero de tweets por fecha y usuario\n",
    "    date_user_counts = df.groupby(['date', 'username']).size().reset_index(name='count')\n",
    "\n",
    "    # Encontrar las 10 fechas con m√°s tweets\n",
    "    top_dates = df['date'].value_counts().nlargest(10).index\n",
    "\n",
    "    # Obtener el usuario con m√°s tweets para cada una de las 10 fechas\n",
    "    result = []\n",
    "    for date_val in top_dates:\n",
    "        top_user_df = date_user_counts[date_user_counts['date'] == date_val].nlargest(1, 'count')\n",
    "        top_user = top_user_df['username'].iloc[0]\n",
    "        result.append((date_val, top_user))\n",
    "\n",
    "    return result\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Optimizaci√≥n en el uso de la memoria:\n",
    "Para este caso vamos a leer directamente el archivo json.\n",
    "* Con top_users_by_date mantenemos un diccionario con el fin de mantener los usuarios con mas tweets\n",
    "* Eliminamos date_counts ya que no se utiliza para los datos finales\n",
    "* Se optimiza la actualizaci√≥n del diccionario top_users_by_date para mantener solo la informaci√≥n necesaria para obtener el usuario con m√°s tweets por fecha."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "from typing import List, Tuple\n",
    "from collections import defaultdict\n",
    "import ujson as json\n",
    "\n",
    "def q1_memory(file_path: str) -> List[Tuple[datetime.date, str]]:\n",
    "    # Diccionario para mantener el conteo de tweets por fecha\n",
    "    date_counts = defaultdict(int)\n",
    "    # Diccionario para mantener el usuario con m√°s tweets por fecha\n",
    "    top_users_by_date = {}\n",
    "    \n",
    "    with open(file_path, \"r\") as file:\n",
    "        for line in file:\n",
    "            tweet = json.loads(line)\n",
    "            # Se extrae la fecha del tweet y se convierte a datetime.date\n",
    "            tweet_date = datetime.strptime(tweet['date'].split(\"T\")[0], '%Y-%m-%d').date()\n",
    "            \n",
    "            # Se actualiza el conteo de tweets para esa fecha\n",
    "            date_counts[tweet_date] += 1\n",
    "            \n",
    "            # Se actualiza el usuario con m√°s tweets para esa fecha\n",
    "            if tweet_date not in top_users_by_date:\n",
    "                top_users_by_date[tweet_date] = defaultdict(int)\n",
    "            top_users_by_date[tweet_date][tweet['user']['username']] += 1\n",
    "    \n",
    "    # Se obtienen las top 10 fechas con m√°s tweets\n",
    "    top_dates = sorted(date_counts, key=date_counts.get, reverse=True)[:10]\n",
    "    \n",
    "    # Se obtiene el usuario con m√°s tweets para cada fecha\n",
    "    result = [(date, max(top_users_by_date[date], key=top_users_by_date[date].get)) for date in top_dates]\n",
    "    \n",
    "    return result\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Los top 10 emojis m√°s usados con su respectivo conteo. \n",
    "\n",
    "Enfoque de tiempo de ejecuci√≥n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import emoji\n",
    "from collections import Counter\n",
    "from typing import List, Tuple\n",
    "\n",
    "def q2_time(file_path: str) -> List[Tuple[str, int]]:\n",
    "    # Se ajusta la ruta de lectura de archivo json a .parquet para futuras lecturas\n",
    "    parquet_file = file_path.replace('.json', '.parquet')\n",
    "    \n",
    "    try:\n",
    "        # Se intenta leer el archivo parquet\n",
    "        df = pd.read_parquet(parquet_file)\n",
    "    except FileNotFoundError:\n",
    "        # Si el archivo parquet no existe, se lee el archivo json y se convierte a parquet\n",
    "        df = pd.read_json(file_path, lines=True)\n",
    "        df.to_parquet(parquet_file, index=False)\n",
    "    \n",
    "    # Se extraen todos los emojis de la columna 'content' y se cuenta su frecuencia\n",
    "    all_emojis = []\n",
    "    for content in df['content']:\n",
    "        emojis_in_content = [entry['emoji'] for entry in emoji.emoji_list(content)]\n",
    "        all_emojis.extend(emojis_in_content)\n",
    "    # Se cuenta la frecuencia de cada emoji\n",
    "    emoji_counts = Counter(all_emojis)\n",
    "\n",
    "    # Se obtienen los top 10 emojis m√°s utilizados\n",
    "    top_emojis = emoji_counts.most_common(10)\n",
    "    \n",
    "    return top_emojis\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Enfoque de memoria optimatizada"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import ujson as json\n",
    "from collections import Counter\n",
    "from typing import List, Tuple\n",
    "import emoji\n",
    "\n",
    "def q2_memory(file_path: str) -> List[Tuple[str, int]]:\n",
    "    # Se crea un contador para contar la frecuencia de cada emoji\n",
    "    emoji_counts = Counter()\n",
    "    \n",
    "    # Se abre el archivo JSON y se itera sobre cada l√≠nea\n",
    "    with open(file_path, 'r', encoding='utf-8') as file:\n",
    "        for line in file:\n",
    "            # Se convierte la l√≠nea en un diccionario\n",
    "            tweet = json.loads(line)\n",
    "            # Se extrae el contenido del tweet y se obtienen los emojis\n",
    "            content = tweet.get('content', '')\n",
    "            emojis_in_content = [entry['emoji'] for entry in emoji.emoji_list(content)]\n",
    "            \n",
    "            # Se actualiza el contador con los emojis encontrados en este tweet\n",
    "            emoji_counts.update(emojis_in_content)\n",
    "\n",
    "    # Se obtienen los top 10 emojis m√°s utilizados\n",
    "    top_emojis = emoji_counts.most_common(10)\n",
    "    \n",
    "    return top_emojis\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### El top 10 hist√≥rico de usuarios (username) m√°s influyentes en funci√≥n del conteo de las menciones (@) que registra cada uno de ellos.\n",
    "\n",
    "Enfoque tiempo de ejecuci√≥n."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "from collections import Counter\n",
    "from typing import List, Tuple\n",
    "\n",
    "def q3_time(file_path: str) -> List[Tuple[str, int]]:\n",
    "    # Se ajusta la ruta de lectura del archivo JSON a .parquet para futuras lecturas\n",
    "    parquet_file = file_path.replace('.json', '.parquet')\n",
    "\n",
    "    try:\n",
    "        # Intentar leer el archivo Parquet si existe\n",
    "        df = pd.read_parquet(parquet_file)\n",
    "    except FileNotFoundError:\n",
    "        try:\n",
    "            # Intentar leer el archivo JSON y convertirlo a DataFrame\n",
    "            df = pd.read_json(file_path, lines=True)\n",
    "            # Procesar los datos y guardarlos como archivo Parquet\n",
    "            df.to_parquet(parquet_file, index=False)\n",
    "        except Exception as e:\n",
    "            print(f\"Error al procesar el archivo: {e}\")\n",
    "            return []\n",
    "\n",
    "    # Se extraen todas las menciones por cada tweet\n",
    "    mentions_flat = []\n",
    "    for mentions_list in df['content'].str.findall(r'@(\\w+)').dropna():\n",
    "        mentions_flat.extend(mentions_list)\n",
    "\n",
    "    # Se cuenta la frecuencia de cada menci√≥n en la lista aplanada\n",
    "    mention_counts = Counter(mentions_flat)\n",
    "\n",
    "    # Obtener los top 10 usuarios m√°s mencionados\n",
    "    top_mentions = mention_counts.most_common(10)\n",
    "\n",
    "    return top_mentions\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Enfoque de memoria optomatizada"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import ujson as json\n",
    "from collections import Counter\n",
    "from typing import List, Tuple\n",
    "import re\n",
    "\n",
    "def q3_memory(file_path: str) -> List[Tuple[str, int]]:\n",
    "    # Se crea un Counter para contar la frecuencia de cada menci√≥n\n",
    "    mention_counts = Counter()\n",
    "    \n",
    "    # Se abre el archivo json y se itera sobre cada tweet\n",
    "    with open(file_path, 'r') as file:\n",
    "        # Iterar l√≠nea por l√≠nea en el archivo\n",
    "        for line in file:\n",
    "            tweet = json.loads(line)\n",
    "            content = tweet.get('content', '')\n",
    "            # Se busca todas las menciones en el contenido y se actualiza el Counter\n",
    "            mention_counts.update(re.findall(r'@(\\w+)', content))\n",
    "            \n",
    "    # Obtener los top 10 usuarios m√°s mencionados\n",
    "    top_mentions = mention_counts.most_common(10)\n",
    "    \n",
    "    return top_mentions\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Resultados\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(datetime.date(2021, 2, 12), 'RanbirS00614606'),\n",
       " (datetime.date(2021, 2, 13), 'MaanDee08215437'),\n",
       " (datetime.date(2021, 2, 17), 'RaaJVinderkaur'),\n",
       " (datetime.date(2021, 2, 16), 'jot__b'),\n",
       " (datetime.date(2021, 2, 14), 'rebelpacifist'),\n",
       " (datetime.date(2021, 2, 18), 'neetuanjle_nitu'),\n",
       " (datetime.date(2021, 2, 15), 'jot__b'),\n",
       " (datetime.date(2021, 2, 20), 'MangalJ23056160'),\n",
       " (datetime.date(2021, 2, 23), 'Surrypuria'),\n",
       " (datetime.date(2021, 2, 19), 'Preetm91')]"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "q1_time(file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(datetime.date(2021, 2, 12), 'RanbirS00614606'),\n",
       " (datetime.date(2021, 2, 13), 'MaanDee08215437'),\n",
       " (datetime.date(2021, 2, 17), 'RaaJVinderkaur'),\n",
       " (datetime.date(2021, 2, 16), 'jot__b'),\n",
       " (datetime.date(2021, 2, 14), 'rebelpacifist'),\n",
       " (datetime.date(2021, 2, 18), 'neetuanjle_nitu'),\n",
       " (datetime.date(2021, 2, 15), 'jot__b'),\n",
       " (datetime.date(2021, 2, 20), 'MangalJ23056160'),\n",
       " (datetime.date(2021, 2, 23), 'Surrypuria'),\n",
       " (datetime.date(2021, 2, 19), 'Preetm91')]"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "q1_memory(file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('üôè', 5049),\n",
       " ('üòÇ', 3072),\n",
       " ('üöú', 2972),\n",
       " ('üåæ', 2182),\n",
       " ('üáÆüá≥', 2086),\n",
       " ('ü§£', 1668),\n",
       " ('‚úä', 1651),\n",
       " ('‚ù§Ô∏è', 1382),\n",
       " ('üôèüèª', 1317),\n",
       " ('üíö', 1040)]"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "q2_time(file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('üôè', 5049),\n",
       " ('üòÇ', 3072),\n",
       " ('üöú', 2972),\n",
       " ('üåæ', 2182),\n",
       " ('üáÆüá≥', 2086),\n",
       " ('ü§£', 1668),\n",
       " ('‚úä', 1651),\n",
       " ('‚ù§Ô∏è', 1382),\n",
       " ('üôèüèª', 1317),\n",
       " ('üíö', 1040)]"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "q2_memory(file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('narendramodi', 2261),\n",
       " ('Kisanektamorcha', 1836),\n",
       " ('RakeshTikaitBKU', 1639),\n",
       " ('PMOIndia', 1422),\n",
       " ('RahulGandhi', 1125),\n",
       " ('GretaThunberg', 1046),\n",
       " ('RaviSinghKA', 1015),\n",
       " ('rihanna', 972),\n",
       " ('UNHumanRights', 962),\n",
       " ('meenaharris', 925)]"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "q3_time(file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('narendramodi', 2261),\n",
       " ('Kisanektamorcha', 1836),\n",
       " ('RakeshTikaitBKU', 1639),\n",
       " ('PMOIndia', 1422),\n",
       " ('RahulGandhi', 1125),\n",
       " ('GretaThunberg', 1046),\n",
       " ('RaviSinghKA', 1015),\n",
       " ('rihanna', 972),\n",
       " ('UNHumanRights', 962),\n",
       " ('meenaharris', 925)]"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "q3_memory(  )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Mejoras referente a la carga del archivo\n",
    "\n",
    "* Si el archivo sigue creciendo sugiero dividir el archivo en lotes con el fin que pueda caber en la memoria RAM, leer y procesar un n√∫mero limitado de registros a la vez en lugar de cargar todo el archivo en la memoria de una vez.\n",
    "* Tener una estructura de datos eficionte lo que nos ayude automatizar el tama√±o del archivo. Por ejemplo que el archivo venga directamente en parquet debido su alamacenamiento columnar lo que reduciria considerablemente el archivo."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pruebas de las funciones"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluacion de memoria q1_memory\n",
      "ERROR: Could not find file C:\\Users\\teddy\\AppData\\Local\\Temp\\ipykernel_90700\\2435509172.py\n",
      "Sun Mar 17 20:51:21 2024    q1_time_stats.pstats\n",
      "\n",
      "         25675 function calls (25107 primitive calls) in 1.083 seconds\n",
      "\n",
      "   Ordered by: cumulative time\n",
      "   List reduced from 684 to 10 due to restriction <10>\n",
      "\n",
      "   ncalls  tottime  percall  cumtime  percall filename:lineno(function)\n",
      "        1    0.000    0.000    1.083    1.083 C:\\Users\\teddy\\AppData\\Local\\Temp\\ipykernel_90700\\1997928156.py:5(q1_time)\n",
      "        1    0.001    0.001    1.022    1.022 c:\\Users\\teddy\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\pandas\\io\\parquet.py:501(read_parquet)\n",
      "        1    0.000    0.000    1.020    1.020 c:\\Users\\teddy\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\pandas\\io\\parquet.py:237(read)\n",
      "        1    0.000    0.000    0.746    0.746 c:\\Users\\teddy\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\pyarrow\\pandas_compat.py:752(table_to_dataframe)\n",
      "        1    0.744    0.744    0.744    0.744 c:\\Users\\teddy\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\pyarrow\\pandas_compat.py:1126(_table_to_blocks)\n",
      "        1    0.000    0.000    0.274    0.274 c:\\Users\\teddy\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\pyarrow\\parquet\\core.py:1759(read_table)\n",
      "        1    0.271    0.271    0.271    0.271 c:\\Users\\teddy\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\pyarrow\\parquet\\core.py:1403(read)\n",
      "        1    0.000    0.000    0.025    0.025 c:\\Users\\teddy\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\pandas\\core\\groupby\\groupby.py:2902(size)\n",
      "        1    0.000    0.000    0.025    0.025 c:\\Users\\teddy\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\pandas\\core\\groupby\\ops.py:684(size)\n",
      "        1    0.000    0.000    0.023    0.023 c:\\Users\\teddy\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\pandas\\core\\groupby\\ops.py:727(group_info)\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from memory_profiler import profile\n",
    "import cProfile\n",
    "import pstats\n",
    "from datetime import datetime\n",
    "\n",
    "# Define tus funciones q1_memory y q1_time aqu√≠\n",
    "\n",
    "@profile\n",
    "def test_memory_usage(file_path):\n",
    "    # Llama a tus funciones aqu√≠\n",
    "    q1_memory(file_path)\n",
    "    q1_time(file_path)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "   \n",
    "    # Prueba de q1_memory\n",
    "    print(\"Evaluacion de memoria q1_memory\")\n",
    "    test_memory_usage(file_path)\n",
    "    \n",
    "    # Prueba de q1_time\n",
    "    profiler = cProfile.Profile()\n",
    "    profiler.enable()\n",
    "    # Codigo a evaluar\n",
    "    q1_time(file_path)\n",
    "    profiler.disable()\n",
    "    profiler.dump_stats(\"q1_time_stats.pstats\")\n",
    "    stats = pstats.Stats(\"q1_time_stats.pstats\")\n",
    "    stats.sort_stats(\"cumulative\")\n",
    "    # Se muestran resultados ordenados por tiempo de ejecucion, del mas lento al mas rapido\n",
    "    stats.print_stats(10)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluacion de memoria q1_memory\n",
      "ERROR: Could not find file C:\\Users\\teddy\\AppData\\Local\\Temp\\ipykernel_90700\\1703600861.py\n",
      "Sun Mar 17 21:09:49 2024    q2_time_stats.pstats\n",
      "\n",
      "         172298652 function calls (172298634 primitive calls) in 37.359 seconds\n",
      "\n",
      "   Ordered by: cumulative time\n",
      "   List reduced from 379 to 10 due to restriction <10>\n",
      "\n",
      "   ncalls  tottime  percall  cumtime  percall filename:lineno(function)\n",
      "   234814    6.043    0.000   34.362    0.000 c:\\Users\\teddy\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\emoji\\core.py:283(emoji_list)\n",
      "      7/3    0.046    0.007   26.366    8.789 c:\\Users\\teddy\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\asyncio\\base_events.py:1908(_run_once)\n",
      " 34281412   16.141    0.000   26.237    0.000 c:\\Users\\teddy\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\emoji\\tokenizer.py:158(tokenize)\n",
      "        8    0.063    0.008   21.014    2.627 c:\\Users\\teddy\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\selectors.py:313(_select)\n",
      "        7    0.555    0.079   18.093    2.585 {built-in method select.select}\n",
      " 34046604    4.167    0.000    7.284    0.000 <string>:1(<lambda>)\n",
      " 34046615    3.117    0.000    3.117    0.000 {built-in method __new__ of type object at 0x00007FFF43B298B0}\n",
      " 34046611    2.765    0.000    2.765    0.000 {method 'append' of 'list' objects}\n",
      "34164276/34164274    2.111    0.000    2.151    0.000 {built-in method builtins.isinstance}\n",
      "        1    0.006    0.006    1.861    1.861 C:\\Users\\teddy\\AppData\\Local\\Temp\\ipykernel_90700\\81624651.py:7(q2_time)\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Define tus funciones q1_memory y q1_time aqu√≠\n",
    "\n",
    "@profile\n",
    "def test_memory_usage(file_path):\n",
    "    # Llama a tus funciones aqu√≠\n",
    "    q2_memory(file_path)\n",
    "    q2_time(file_path)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "   \n",
    "    # Prueba de q1_memory\n",
    "    print(\"Evaluacion de memoria q1_memory\")\n",
    "    test_memory_usage(file_path)\n",
    "    \n",
    "    # Prueba de q1_time\n",
    "    profiler = cProfile.Profile()\n",
    "    profiler.enable()\n",
    "    # Codigo a evaluar\n",
    "    q2_time(file_path)\n",
    "    q2_memory(file_path)\n",
    "    profiler.disable()\n",
    "    profiler.dump_stats(\"q2_time_stats.pstats\")\n",
    "    stats = pstats.Stats(\"q2_time_stats.pstats\")\n",
    "    stats.sort_stats(\"cumulative\")\n",
    "    # Se muestran resultados ordenados por tiempo de ejecucion, del mas lento al mas rapido\n",
    "    stats.print_stats(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluacion de memoria q1_memory\n",
      "ERROR: Could not find file C:\\Users\\teddy\\AppData\\Local\\Temp\\ipykernel_90700\\2046086110.py\n",
      "Sun Mar 17 21:10:28 2024    q3_time_stats.pstats\n",
      "\n",
      "         1510390 function calls (1510372 primitive calls) in 3.683 seconds\n",
      "\n",
      "   Ordered by: cumulative time\n",
      "   List reduced from 443 to 10 due to restriction <10>\n",
      "\n",
      "   ncalls  tottime  percall  cumtime  percall filename:lineno(function)\n",
      "        1    0.012    0.012    2.059    2.059 C:\\Users\\teddy\\AppData\\Local\\Temp\\ipykernel_90700\\1467250520.py:6(q3_time)\n",
      "        1    0.001    0.001    1.992    1.992 c:\\Users\\teddy\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\pandas\\io\\parquet.py:501(read_parquet)\n",
      "        1    0.000    0.000    1.991    1.991 c:\\Users\\teddy\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\pandas\\io\\parquet.py:237(read)\n",
      "        1    0.000    0.000    1.587    1.587 c:\\Users\\teddy\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\pyarrow\\pandas_compat.py:752(table_to_dataframe)\n",
      "        1    1.586    1.586    1.586    1.586 c:\\Users\\teddy\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\pyarrow\\pandas_compat.py:1126(_table_to_blocks)\n",
      "   117407    0.991    0.000    0.991    0.000 {built-in method ujson.loads}\n",
      "        1    0.000    0.000    0.403    0.403 c:\\Users\\teddy\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\pyarrow\\parquet\\core.py:1759(read_table)\n",
      "        1    0.400    0.400    0.400    0.400 c:\\Users\\teddy\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\pyarrow\\parquet\\core.py:1403(read)\n",
      "        2    0.000    0.000    0.379    0.190 c:\\Users\\teddy\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\asyncio\\base_events.py:1908(_run_once)\n",
      "        2    0.000    0.000    0.379    0.189 c:\\Users\\teddy\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\selectors.py:319(select)\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Define tus funciones q1_memory y q1_time aqu√≠\n",
    "\n",
    "@profile\n",
    "def test_memory_usage(file_path):\n",
    "    # Llama a tus funciones aqu√≠\n",
    "    q3_memory(file_path)\n",
    "    q3_time(file_path)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "   \n",
    "    # Prueba de q1_memory\n",
    "    print(\"Evaluacion de memoria q1_memory\")\n",
    "    test_memory_usage(file_path)\n",
    "    \n",
    "    # Prueba de q1_time\n",
    "    profiler = cProfile.Profile()\n",
    "    profiler.enable()\n",
    "    # Codigo a evaluar\n",
    "    q3_time(file_path)\n",
    "    q3_memory(file_path)\n",
    "    profiler.disable()\n",
    "    profiler.dump_stats(\"q3_time_stats.pstats\")\n",
    "    stats = pstats.Stats(\"q3_time_stats.pstats\")\n",
    "    stats.sort_stats(\"cumulative\")\n",
    "    # Se muestran resultados ordenados por tiempo de ejecucion, del mas lento al mas rapido\n",
    "    stats.print_stats(10)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
